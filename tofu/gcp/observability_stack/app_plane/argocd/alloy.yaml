
# Source: alloy/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-config
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: config
data:
  config.alloy: |-
    prometheus.remote_write "default" {
      endpoint {
        url = "http://prometheus-server.observability.svc.cluster.local:9090/api/v1/write"
      }
    }

    logging {
      level  = "info"
      format = "logfmt"
    }

    livedebugging {
      enabled = true
    }

    import.git "ksm" {
      repository = "https://github.com/grafana/alloy-modules.git"
      revision = "main"
      path = "modules/kubernetes/kube-state-metrics/metrics.alloy"
      pull_frequency = "15m"
    }

    prometheus.operator.podmonitors "pods" {
      forward_to = [prometheus.relabel.drop_unwanted_metrics.receiver]

      namespaces = ["observability"]
    }

    discovery.kubernetes "pods" {
      role = "pod"
    }

    discovery.kubernetes "nodes" {
      role = "node"
    }

    ksm.kubernetes "targets" {}

    prometheus.exporter.unix "node_exporter" {}

    prometheus.exporter.self "alloy" {}

    discovery.relabel "metrics_cadvisor" {
      targets = discovery.kubernetes.nodes.targets

      rule {
        action = "labelmap"
        regex = "__meta_kubernetes_node_label_(.+)"
      }

      rule {
        action       = "replace"
        target_label = "__address__"
        replacement  = "kubernetes.default.svc.cluster.local:443"
      }

      rule {
        source_labels = ["__meta_kubernetes_node_name"]
        regex         = "(.+)"
        action        = "replace"
        replacement   = "/api/v1/nodes/${1}/proxy/metrics/cadvisor"
        target_label  = "__metrics_path__"
      }
    }

    discovery.relabel "metrics_kubelet" {
      targets = discovery.kubernetes.nodes.targets

      rule {
        action       = "replace"
        target_label = "__address__"
        replacement  = "kubernetes.default.svc.cluster.local:443"
      }

      rule {
        source_labels = ["__meta_kubernetes_node_name"]
        regex         = "(.+)"
        action        = "replace"
        replacement   = "/api/v1/nodes/${1}/proxy/metrics"
        target_label  = "__metrics_path__"
      }

    }

    prometheus.scrape "pods" {
      scheme = "https"

      tls_config {
        server_name          = "kubernetes"
        ca_file              = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
        insecure_skip_verify = false
      }
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      targets           = discovery.kubernetes.pods.targets
      scrape_interval   = "60s"
      forward_to        = [prometheus.relabel.drop_unwanted_metrics.receiver]
    }

    prometheus.scrape "cadvisor" {
      scheme = "https"

      tls_config {
        server_name          = "kubernetes"
        ca_file              = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
        insecure_skip_verify = false
      }
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      targets           = discovery.relabel.metrics_cadvisor.output
      scrape_interval   = "60s"
      forward_to        = [prometheus.relabel.drop_unwanted_metrics.receiver]
    }

    prometheus.scrape "kubelet" {
      scheme = "https"

      tls_config {
        server_name          = "kubernetes"
        ca_file              = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
        insecure_skip_verify = false
      }
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      targets           = discovery.relabel.metrics_kubelet.output
      scrape_interval   = "60s"
      forward_to        = [prometheus.relabel.drop_unwanted_metrics.receiver]
    }

    prometheus.scrape "node_exporter" {
      targets = prometheus.exporter.unix.node_exporter.targets
      forward_to = [prometheus.relabel.node_exporter.receiver]

      job_name = "node-exporter"
    }

    ksm.scrape "metrics" {
      targets = ksm.kubernetes.targets.output
      forward_to = [prometheus.relabel.kube_state_metrics.receiver]
    }

    prometheus.scrape "alloy" {
      targets    = prometheus.exporter.self.alloy.targets
      forward_to = [prometheus.relabel.drop_unwanted_metrics.receiver]
    }

    prometheus.scrape "prometheus" {
      targets = [
        {__address__ = "prometheus-server.observability.svc.cluster.local:9090"},
      ]
      forward_to = [prometheus.relabel.drop_unwanted_metrics.receiver]
    }

    prometheus.relabel "kube_state_metrics" {
      forward_to = [prometheus.relabel.drop_unwanted_metrics.receiver]

      rule {
        replacement = env("HOSTNAME")
        target_label = "nodename"
      }

      rule {
        replacement = "kube-state-metrics"
        target_label = "job"
      }
    }

    prometheus.relabel "drop_unwanted_metrics" {
      forward_to = [prometheus.relabel.add_cluster_label.receiver]

      rule {
        source_labels = ["__name__"]
        regex         = "^(___customer_|kube_node_info|kube_pod_info|kube_pod_status_phase|kube_pod_container_|prometheus_build_info|prometheus_tsdb_head_series|up|container_fs_usage_bytes|kubelet_volume_stats_used_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_inodes_used|kubelet_volume_stats_inodes|container_network_receive_packets_total|container_network_transmit_packets_total|container_network_receive_packets_dropped_total|container_network_transmit_packets_dropped_total|container_network_receive_errors_total|container_network_transmit_errors_total|kube_namespace_created|container_oom_events_total|container_cpu_cfs_throttled_seconds_total|kubelet_volume_stats_used_bytes|node_cpu_core_throttles_total|container_fs_limit_bytes|machine_cpu_cores|machine_memory_bytes|container_memory_usage_bytes|container_memory_rss|container_memory_working_set_bytes|container_cpu_usage_seconds_total|container_network_receive_bytes_total|container_network_transmit_bytes_total|kubelet_container_status_phase|kubelet_pod_status_phase).*"
        action        = "keep"
      }

      rule {
        replacement = env("HOSTNAME")
        target_label = "nodename"
      }
    }

    prometheus.relabel "node_exporter" {
      forward_to = [prometheus.relabel.add_cluster_label.receiver]

      rule {
        replacement = env("HOSTNAME")
        target_label = "nodename"
      }

      rule {
        replacement = "node-exporter"
        target_label = "job"
      }
    }

    prometheus.relabel "add_cluster_label" {
      forward_to = [prometheus.remote_write.default.receiver]

      rule {
        // Select "node" label and extract the cluster name with regex
        source_labels = ["nodename", "node"]
        regex = "gke-(c-[^-]+).+"
        target_label = "cluster"
        replacement = "$1"
        action = "replace"
      }
    }
---
# Source: alloy/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alloy
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
---
# Source: alloy/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: alloy
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
rules:
  # Rules which allow discovery.kubernetes to function.
  - apiGroups:
      - ""
      - "discovery.k8s.io"
      - "networking.k8s.io"
    resources:
      - endpoints
      - endpointslices
      - ingresses
      - nodes
      - nodes/proxy
      - nodes/metrics
      - pods
      - services
    verbs:
      - get
      - list
      - watch
  # Rules which allow loki.source.kubernetes and loki.source.podlogs to work.
  - apiGroups:
      - ""
    resources:
      - pods
      - pods/log
      - namespaces
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "monitoring.grafana.com"
    resources:
      - podlogs
    verbs:
      - get
      - list
      - watch
  # Rules which allow mimir.rules.kubernetes to work.
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - prometheusrules
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - /metrics
    verbs:
      - get
  # Rules for prometheus.kubernetes.*
  - apiGroups: ["monitoring.coreos.com"]
    resources:
      - podmonitors
      - servicemonitors
      - probes
    verbs:
      - get
      - list
      - watch
  # Rules which allow eventhandler to work.
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - get
      - list
      - watch
  # needed for remote.kubernetes.*
  - apiGroups: [""]
    resources:
      - "configmaps"
      - "secrets"
    verbs:
      - get
      - list
      - watch
  # needed for otelcol.processor.k8sattributes
  - apiGroups: ["apps"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["extensions"]
    resources: ["replicasets"]
    verbs: ["get", "list", "watch"]
---
# Source: alloy/templates/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alloy
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: rbac
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alloy
subjects:
  - kind: ServiceAccount
    name: alloy
    namespace: observability
---
# Source: alloy/templates/cluster_service.yaml
apiVersion: v1
kind: Service
metadata:
  name: alloy-cluster
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  clusterIP: "None"
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy
  ports:
    # Do not include the -metrics suffix in the port name, otherwise metrics
    # can be double-collected with the non-headless Service if it's also
    # enabled.
    #
    # This service should only be used for clustering, and not metric
    # collection.
    - name: http
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: alloy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: alloy
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
    app.kubernetes.io/component: networking
spec:
  type: ClusterIP
  selector:
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy
  internalTrafficPolicy: Cluster
  ports:
    - name: http-metrics
      port: 12345
      targetPort: 12345
      protocol: "TCP"
---
# Source: alloy/templates/controllers/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: alloy
  namespace: observability
  labels:
    helm.sh/chart: alloy-0.10.0
    app.kubernetes.io/name: alloy
    app.kubernetes.io/instance: alloy

    app.kubernetes.io/version: "v1.5.0"
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: alloy
spec:
  replicas: 1
  podManagementPolicy: Parallel
  minReadySeconds: 10
  serviceName: alloy
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy
      app.kubernetes.io/instance: alloy
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: alloy
      labels:
        app.kubernetes.io/name: alloy
        app.kubernetes.io/instance: alloy
    spec:
      serviceAccountName: alloy
      containers:
        - name: alloy
          image: docker.io/grafana/alloy:v1.5.0
          imagePullPolicy: IfNotPresent
          args:
            - run
            - /etc/alloy/config.alloy
            - --storage.path=/tmp/alloy
            - --server.http.listen-addr=0.0.0.0:12345
            - --server.http.ui-path-prefix=/
            - --cluster.enabled=true
            - --cluster.join-addresses=alloy-cluster
            - --cluster.name="dev-cluster-1"
            - --stability.level=experimental
          env:
            - name: ALLOY_DEPLOY_MODE
              value: "helm"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-metrics
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 12345
              scheme: HTTP
            initialDelaySeconds: 10
            timeoutSeconds: 1
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
        - name: config-reloader
          image: ghcr.io/jimmidyson/configmap-reload:v0.12.0
          args:
            - --volume-dir=/etc/alloy
            - --webhook-url=http://localhost:12345/-/reload
          volumeMounts:
            - name: config
              mountPath: /etc/alloy
          resources:
            requests:
              cpu: 1m
              memory: 5Mi
      dnsPolicy: ClusterFirst
      nodeSelector:
        node_pool: observability
      volumes:
        - name: config
          configMap:
            name: alloy-config
