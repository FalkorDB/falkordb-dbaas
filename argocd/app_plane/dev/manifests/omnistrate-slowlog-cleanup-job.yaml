---
# Kubernetes CronJob for executing bgrewriteaof on FalkorDB instance-* namespaces
# This CronJob scans for namespaces matching the pattern "instance-*" and creates
# individual Jobs to perform background AOF rewriting on FalkorDB node pods.

apiVersion: v1
kind: Namespace
metadata:
  name: slowlog-cleanup-job
  labels:
    app: slowlog-cleanup-job

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: slowlog-cleanup-scanner-script
  namespace: slowlog-cleanup-job
data:
  namespace-scanner.sh: |
    #!/bin/bash
    set -e

    # Configuration
    NAMESPACE_PATTERN="${NAMESPACE_PATTERN:-instance-}"
    MAX_CONCURRENT_JOBS="${MAX_CONCURRENT_JOBS:-1}"

    echo "Starting namespace scan for ${NAMESPACE_PATTERN}* pattern..."

    # Get all namespaces matching the pattern
    INSTANCE_NAMESPACES=$(kubectl get namespaces -o name | grep "namespace/${NAMESPACE_PATTERN}" | sed 's/namespace\///')

    if [ -z "$INSTANCE_NAMESPACES" ]; then
      echo "No namespaces found matching '${NAMESPACE_PATTERN}*' pattern"
      exit 0
    fi

    echo "Found namespaces: $INSTANCE_NAMESPACES"

    # Count current running jobs to avoid overwhelming the cluster
    CURRENT_JOBS=$(kubectl get jobs -l app=bgrewriteaof-job --field-selector status.successful!=1 -o name | wc -l)

    if [ "$CURRENT_JOBS" -ge "$MAX_CONCURRENT_JOBS" ]; then
      echo "Maximum concurrent jobs ($MAX_CONCURRENT_JOBS) reached. Skipping this run."
      exit 0
    fi

    # For each instance namespace, create a job
    for namespace in $INSTANCE_NAMESPACES; do

      # Get all pods in the namespace
      PODS=$(kubectl get pods -n $namespace -o json 2>/dev/null || echo '{"items":[]}')
      
      if [ "$PODS" = '{"items":[]}' ]; then
        echo "No pods found in namespace $namespace"
        continue
      fi
      
      # Filter pods with RUN_NODE=1
      NODE_PODS=$(echo "$PODS" | jq -r '.items[] | select(
        ([.spec.containers[]?.env[]? | select(.name == "RUN_NODE" and .value == "1")] | length > 0)
      ) | .metadata.name' | sort | uniq)
      
      if [ -z "$NODE_PODS" ]; then
        echo "No FalkorDB node pods found in namespace $namespace"
        continue
      fi

      echo "Creating slowlog-cleanup job for namespace: $namespace"
      
      # Generate unique job name
      JOB_NAME="slowlog-cleanup-$namespace-$(date +%s)"

      # Create the job manifest
      cat <<EOF | kubectl apply -f -
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: $JOB_NAME
      namespace: slowlog-cleanup-job
      labels:
        app: slowlog-cleanup-job
        target-namespace: $namespace
        created-by: slowlog-cleanup-cronjob
    spec:
      ttlSecondsAfterFinished: 300
      template:
        spec:
          serviceAccountName: instance-kubectl-exec-sa
          restartPolicy: Never
          containers:
          - name: slowlog-cleanup-executor
            image: bitnamilegacy/kubectl:latest
            securityContext:
              runAsNonRoot: true
              runAsUser: 1000
              allowPrivilegeEscalation: false
            env:
            - name: TARGET_NAMESPACE
              value: "$namespace"
            volumeMounts:
            - name: slowlog-cleanup-script
              mountPath: /scripts
            command:
            - /bin/bash
            - /scripts/slowlog-cleanup.sh
          volumes:
          - name: slowlog-cleanup-script
            configMap:
              name: slowlog-cleanup-execution-script
              defaultMode: 0755
    EOF
      
      echo "Job $JOB_NAME created successfully"
    done

    echo "All slowlog-cleanup jobs created successfully"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: slowlog-cleanup-execution-script
  namespace: slowlog-cleanup-job
data:
  slowlog-cleanup.sh: |
    #!/bin/bash
    set -e

    echo "Starting slowlog-cleanup execution for namespace: $TARGET_NAMESPACE"

    # Get all pods in the namespace
    PODS=$(kubectl get pods -n $TARGET_NAMESPACE -o json 2>/dev/null || echo '{"items":[]}')

    if [ "$PODS" = '{"items":[]}' ]; then
      echo "No pods found in namespace $TARGET_NAMESPACE"
      exit 0
    fi

    # Filter pods with RUN_NODE=1
    NODE_PODS=$(echo "$PODS" | jq -r '.items[] | select(
      ([.spec.containers[]?.env[]? | select(.name == "RUN_NODE" and .value == "1")] | length > 0)
    ) | .metadata.name' | sort | uniq)

    if [ -z "$NODE_PODS" ]; then
      echo "No FalkorDB node pods found in namespace $TARGET_NAMESPACE"
      exit 0
    fi

    echo "Found FalkorDB node pods in $TARGET_NAMESPACE: $NODE_PODS"

    for POD_NAME in $NODE_PODS; do
      echo "Processing pod: $POD_NAME"

      # Check if pod is running
      POD_STATUS=$(kubectl get pod -n $TARGET_NAMESPACE $POD_NAME -o jsonpath='{.status.phase}')
      if [ "$POD_STATUS" != "Running" ]; then
        echo "Pod $POD_NAME is not running (status: $POD_STATUS), skipping..."
        continue
      fi

      # Get Redis password from secret file inside the first pod
      if [[ -z "$PASSWORD" ]]; then
        PASSWORD=$(kubectl exec -n $TARGET_NAMESPACE $POD_NAME -c service -- cat /run/secrets/adminpassword)
      fi

      # Check pod's role
      ROLE=$(kubectl exec -n $TARGET_NAMESPACE $POD_NAME -c service -- \
        redis-cli -a "$PASSWORD" INFO replication | grep '^role:' | cut -d: -f2 | tr -d '\r')
      
      if [ "$ROLE" != "master" ]; then
        echo "Pod $POD_NAME is not a master (role: $ROLE), skipping..."
        continue
      fi

      # Delete telemetry* keys
      kubectl exec -n $TARGET_NAMESPACE $POD_NAME -c service -- \
        redis-cli -a "$PASSWORD" -h "$POD_NAME" --scan --pattern "telemetry*" \
        | xargs -r -I {} kubectl exec -n $TARGET_NAMESPACE $POD_NAME -c service -- \
          redis-cli -a "$PASSWORD" -h "$POD_NAME" UNLINK {}

      # Reset GRAPH.SLOWLOG for all keys
      kubectl exec -n $TARGET_NAMESPACE $POD_NAME -c service -- \
        redis-cli -a "$PASSWORD" -h "$POD_NAME" --scan \
        | xargs -r -I {} kubectl exec -n $TARGET_NAMESPACE $POD_NAME -c service -- \
          redis-cli -a "$PASSWORD" -h "$POD_NAME" GRAPH.SLOWLOG {} RESET

    done

    echo "Slowlog-cleanup execution process completed for namespace $TARGET_NAMESPACE"

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: instance-kubectl-exec-sa
  namespace: slowlog-cleanup-job
  labels:
    app: instance-kubectl-exec

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: slowlog-cleanup-kubectl-exec-role
  labels:
    app: slowlog-cleanup-kubectl-exec
rules:
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["list", "get"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["list", "get"]
  - apiGroups: [""]
    resources: ["pods/exec"]
    verbs: ["create"]
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "get", "list", "delete"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: slowlog-cleanup-kubectl-exec-binding
  labels:
    app: slowlog-cleanup-kubectl-exec
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: slowlog-cleanup-kubectl-exec-role
subjects:
  - kind: ServiceAccount
    name: instance-kubectl-exec-sa
    namespace: slowlog-cleanup-job

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: slowlog-cleanup-cronjob
  namespace: slowlog-cleanup-job
  labels:
    app: slowlog-cleanup
spec:
  # Run every 6 hours
  schedule: "0 */6 * * *"
  concurrencyPolicy: Forbid
  failedJobsHistoryLimit: 3
  successfulJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: slowlog-cleanup-scanner
        spec:
          serviceAccountName: instance-kubectl-exec-sa
          restartPolicy: OnFailure
          containers:
            - name: namespace-scanner
              image: bitnamilegacy/kubectl:latest
              securityContext:
                runAsNonRoot: true
                runAsUser: 1000
                allowPrivilegeEscalation: false
              resources:
                requests:
                  memory: "64Mi"
                  cpu: "50m"
                limits:
                  memory: "128Mi"
                  cpu: "100m"
              env:
                - name: NAMESPACE_PATTERN
                  value: "instance-"
                - name: MAX_CONCURRENT_JOBS
                  value: "1"
              volumeMounts:
                - name: scanner-script
                  mountPath: /scripts
              command:
                - /bin/bash
                - /scripts/namespace-scanner.sh
          volumes:
            - name: scanner-script
              configMap:
                name: slowlog-cleanup-scanner-script
                defaultMode: 0755
